{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/AudioToText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# ðŸ—£ï¸ [**AudioToText**](https://github.com/Carleslc/AudioToText)\n",
        "\n",
        "[![Donate](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/carleslc)\n",
        "\n",
        "### ðŸ›  [Whisper by OpenAI](https://github.com/openai/whisper)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_lylR1xWMxk"
      },
      "source": [
        "## [Step 1] âš™ï¸ Install the required libraries\n",
        "\n",
        "Click â–¶ï¸ button below to install the dependencies for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SJl7HJOeo0-P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Install ffmpeg: https://ffmpeg.org/download.html\n"
          ]
        }
      ],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "import subprocess\n",
        "\n",
        "from sys import platform as sys_platform\n",
        "\n",
        "status, ffmpeg_version = subprocess.getstatusoutput(\"ffmpeg -version\")\n",
        "\n",
        "if status != 0:\n",
        "  from platform import platform\n",
        "\n",
        "  if sys_platform == 'linux' and 'ubuntu' in platform().lower():\n",
        "    !apt install ffmpeg\n",
        "  else:\n",
        "    print(\"Install ffmpeg: https://ffmpeg.org/download.html\")\n",
        "else:\n",
        "  print(ffmpeg_version.split('\\n')[0])\n",
        "\n",
        "  NO_ROOT_WARNING = '|& grep -v \\\"WARNING: Running pip as the \\'root\\' user\"' # running in Colab\n",
        "\n",
        "  !pip install --no-warn-script-location --user --upgrade pip {NO_ROOT_WARNING}\n",
        "  !pip install --root-user-action=ignore git+https://github.com/openai/whisper.git@v20231117 openai==1.9.0 numpy scipy deepl pydub cohere ffmpeg-python torch==2.1.0 tensorflow-probability==0.23.0 typing-extensions==4.9.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5A5bTMB8XmtI"
      },
      "source": [
        "## [Step 2] ðŸ“ Upload your audio files to the Files folder\n",
        "\n",
        "â¬…ï¸ Files folder in Google Colab is on the left menu\n",
        "\n",
        "Almost any audio or video file format is [supported](https://gist.github.com/Carleslc/1d6b922c8bf4a7e9627a6970d178b3a6)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-9_I0W3tqTjr"
      },
      "source": [
        "## [Step 3] ðŸ‘‚ Transcribe or Translate\n",
        "\n",
        "3.1. Choose a `task`:\n",
        "  - `Transcribe` speech to text in the same language of the source audio file.\n",
        "  - `Translate to English` speech to text in English.\n",
        "  \n",
        "Translation to other languages is not supported with _Whisper_ by default.\n",
        "You may try to choose the _Transcribe_ task and set your desired `language`, but translation is not guaranteed. However, you can use **_DeepL_** later in the Step 5 to translate the transcription to another language.\n",
        "\n",
        "3.2. Edit the `audio_file` to match your uploaded file name to transcribe.\n",
        "\n",
        "- If you want to transcribe multiple files with the same parameters you must separate their file names with commas `,`\n",
        "\n",
        "3.3. Run this cell and wait for the transcription to complete.\n",
        "\n",
        "  You can try other parameters if the result with default parameters does not suit your needs.\n",
        "\n",
        "  [Available models and languages](https://github.com/openai/whisper#available-models-and-languages)\n",
        "\n",
        "  Setting the `language` to the language of source audio file may provide better results than Auto-Detect.\n",
        "\n",
        "  You can add an optional initial `prompt` to provide context about the audio or encourage a specific writing style, see the [prompting guide](https://platform.openai.com/docs/guides/speech-to-text/prompting).\n",
        "\n",
        "  If the execution takes too long to complete you can choose a smaller model in `use_model`, with an accuracy tradeoff, or use the OpenAI API.\n",
        "\n",
        "  By default the open-source models are used, but you can also use the OpenAI API if the `api_key` parameter is set with your [OpenAI API Key](https://platform.openai.com/account/api-keys), which can improve the inference speed substantially, but it has an associated cost, see [API pricing](https://openai.com/pricing#audio-models).\n",
        "\n",
        "  When using API some options are fixed: **use_model** is ignored (uses _large-v2_) and **coherence_preference** is ignored (uses _More coherence_).\n",
        "  \n",
        "  More parameters are available in the code `options` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F:\\DATA\\Femdom\\AlexandraSnow\\Black_Latex_Joi-H265.m4v,F:\\DATA\\Femdom\\AlexandraSnow\\More_Shiny_Tit_Worship-H265.m4v,F:\\DATA\\Femdom\\AlexandraSnow\\Shiny_Tit_Tease-H265.m4v\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# This cell define the input file path\n",
        "file_path = [\"F:\\DATA\\Femdom\\AlexandraSnow\\Black_Latex_Joi-H265.m4v\",\n",
        "\"F:\\DATA\\Femdom\\AlexandraSnow\\More_Shiny_Tit_Worship-H265.m4v\",\n",
        "\"F:\\DATA\\Femdom\\AlexandraSnow\\Shiny_Tit_Tease-H265.m4v\"]\n",
        "if type(file_path) == str:\n",
        "    print('File_path is a string')\n",
        "    file_path_new = file_path.replace(' ', '_') # remove spaces to make life easier\n",
        "    file_path_new = file_path_new.replace('(', '_')\n",
        "    file_path_new = file_path_new.replace(')', '_')\n",
        "    file_path_new = file_path_new.replace(',', '') # remove commas to avoid issues\n",
        "    os.rename(file_path, file_path_new) # rename file to avoid issues\n",
        "else:\n",
        "    file_path_list = file_path.copy()\n",
        "    file_path_new = ''\n",
        "    for i in range(len(file_path)):\n",
        "        file_path_list[i] = file_path[i].replace(' ', '_')\n",
        "        file_path_list[i] = file_path_list[i].replace('(', '_')\n",
        "        file_path_list[i] = file_path_list[i].replace(')', '_')\n",
        "        file_path_list[i] = file_path_list[i].replace(',', '') # remove commas to avoid issues\n",
        "        os.rename(file_path[i], file_path_list[i]) # rename file to avoid issues\n",
        "    file_path_new = \",\".join(file_path_list)\n",
        "print(file_path_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F:\\DATA\\Femdom\\AlexandraSnow\\Black_Latex_Joi-H265.m4v,F:\\DATA\\Femdom\\AlexandraSnow\\More_Shiny_Tit_Worship-H265.m4v,F:\\DATA\\Femdom\\AlexandraSnow\\Shiny_Tit_Tease-H265.m4v\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def get_m4v_file_paths(directory):\n",
        "    # List all .m4v files in the specified directory with full paths\n",
        "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.m4v') and os.path.isfile(os.path.join(directory, f))]\n",
        "    # Join the file paths into a single string separated by commas\n",
        "    file_paths = \",\".join(files)\n",
        "    return file_paths\n",
        "\n",
        "# This cell define the input file path\n",
        "file_path_new = get_m4v_file_paths(\"F:\\DATA\\Femdom\\AlexandraSnow\")\n",
        "print(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "opNkn_Lgpat4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "GPU 0: NVIDIA GeForce RTX 3070 (UUID: GPU-89b0531b-02ee-f190-239a-23d742684541)\n",
            "\n",
            "Language: English\n",
            "\n",
            "Loading medium.en model... C:\\Users\\kentc/.cache/whisper/medium.en.pt\n",
            "Model medium.en is English-only and has 762,320,896 parameters.\n",
            "\n",
            "-- TRANSCRIPTION --\n",
            "\n",
            "Processing: F:\\DATA\\Femdom\\AlexandraSnow\\Black_Latex_Joi-H265.m4v\n",
            "\n",
            "[00:00.000 --> 00:14.000]  I know you find me very beautiful, don't you?\n",
            "[00:14.000 --> 00:25.000]  I know that you've been waiting for an opportunity to worship me.\n",
            "[00:25.000 --> 00:35.000]  Especially wearing this.\n",
            "[00:35.000 --> 00:54.000]  What is it about this tight, shiny rubber\n",
            "[00:54.000 --> 00:58.000]  that really gets you off?\n",
            "[00:58.000 --> 01:04.000]  What is it? Do you even know?\n",
            "[01:04.000 --> 01:07.000]  Is it the smell?\n",
            "[01:07.000 --> 01:12.000]  Imagine me leaning into you.\n",
            "[01:12.000 --> 01:18.000]  You could smell the rubber as it\n",
            "[01:18.000 --> 01:27.000]  wrapped around my chest.\n",
            "[01:27.000 --> 01:30.000]  Perhaps it's just a look.\n",
            "[01:30.000 --> 01:37.000]  It's super tight, super shiny.\n",
            "[01:37.000 --> 01:45.000]  Wrapped around every curve.\n",
            "[01:45.000 --> 01:50.000]  Perhaps it's just a look.\n",
            "[01:50.000 --> 01:58.000]  Knowing all this tight, shiny rubber means.\n",
            "[01:58.000 --> 02:04.000]  I'm the real thing.\n",
            "[02:04.000 --> 02:07.000]  And I'm going to fuck you up.\n",
            "[02:07.000 --> 02:13.000]  And you know it.\n",
            "[02:13.000 --> 02:29.000]  Like I stepped off the page as your favorite comic book, hm?\n",
            "[02:29.000 --> 02:35.000]  So go ahead and start.\n",
            "[02:35.000 --> 02:41.000]  Yeah, you're already doing it.\n",
            "[02:41.000 --> 02:43.000]  Stupid.\n",
            "[02:43.000 --> 02:48.000]  My gorgeous dress.\n",
            "[02:48.000 --> 02:50.000]  It's still part of it.\n",
            "[02:50.000 --> 02:52.000]  It's still part of it.\n",
            "[02:52.000 --> 02:55.000]  It's still, it's still.\n",
            "[02:55.000 --> 02:57.000]  Hm.\n",
            "[02:57.000 --> 02:59.000]  I don't have so much time I would have to give you.\n",
            "[02:59.000 --> 03:02.000]  Maybe I might even go let you come.\n",
            "[03:02.000 --> 03:10.000]  Hm.\n",
            "[03:10.000 --> 03:11.000]  Hm.\n",
            "[03:11.000 --> 03:22.000]  Perhaps I should sell it more.\n",
            "[03:22.000 --> 03:31.000]  Nice, shiny, sleek, hm.\n",
            "[03:31.000 --> 03:35.000]  Put you stroking harder and longer.\n",
            "[03:35.000 --> 03:37.000]  So excited.\n",
            "[03:37.000 --> 03:42.000]  I love that you've got the semantics.\n",
            "[03:42.000 --> 03:49.000]  It's such a treat for you too, I don't worry about it.\n",
            "[03:50.000 --> 03:52.000]  It's such a treat.\n",
            "[03:52.000 --> 03:54.000]  It's your stroke.\n",
            "[03:54.000 --> 03:56.000]  Stroking means I touch myself.\n",
            "[03:56.000 --> 03:58.000]  I don't love the way it looks.\n",
            "[03:58.000 --> 04:01.000]  I have to know.\n",
            "[04:01.000 --> 04:13.000]  What does to my body?\n",
            "[04:14.000 --> 04:18.000]  I know you love my ass looks so good.\n",
            "[04:18.000 --> 04:23.000]  Hm.\n",
            "[04:23.000 --> 04:28.000]  I must reach out.\n",
            "[04:28.000 --> 04:31.000]  Polishing of your tongue.\n",
            "[04:31.000 --> 04:34.000]  Hm.\n",
            "[04:34.000 --> 04:40.000]  Polishing of your tongue.\n",
            "[04:40.000 --> 04:44.000]  I love the way the light shines off of it.\n",
            "[04:44.000 --> 04:54.000]  Hm.\n",
            "[04:54.000 --> 04:55.000]  What's that?\n",
            "[04:55.000 --> 04:57.000]  Stroke some more.\n",
            "[04:57.000 --> 04:59.000]  Stroke me harder.\n",
            "[04:59.000 --> 05:01.000]  Harder.\n",
            "[05:01.000 --> 05:02.000]  I know you love me.\n",
            "[05:02.000 --> 05:04.000]  I'm not stroking for me.\n",
            "[05:04.000 --> 05:07.000]  I know you love masturbating to my image.\n",
            "[05:07.000 --> 05:08.000]  Of course you do.\n",
            "[05:08.000 --> 05:11.000]  How could you not?\n",
            "[05:11.000 --> 05:13.000]  Hm.\n",
            "[05:13.000 --> 05:16.000]  I enjoy knowing it like harpovers.\n",
            "[05:16.000 --> 05:19.000]  I enjoy knowing all of your emotions.\n",
            "[05:19.000 --> 05:23.000]  Making you hopelessly dependent on my image.\n",
            "[05:23.000 --> 05:27.000]  Just to be able to masturbate.\n",
            "[05:27.000 --> 05:32.000]  Just to be able to come.\n",
            "[05:32.000 --> 05:35.000]  Hm.\n",
            "[05:35.000 --> 05:38.000]  All my reason.\n",
            "[05:38.000 --> 05:40.000]  Do you even get on the internet anymore?\n",
            "[05:40.000 --> 05:41.000]  Do you even check your email?\n",
            "[05:41.000 --> 05:47.000]  How do you feel when I get to my videos?\n",
            "[05:47.000 --> 05:51.000]  Hm.\n",
            "[05:51.000 --> 05:54.000]  Stroke me harder, Mel.\n",
            "[05:54.000 --> 05:55.000]  Harder.\n",
            "[05:55.000 --> 05:57.000]  Faster.\n",
            "[05:57.000 --> 05:59.000]  Hm.\n",
            "[05:59.000 --> 06:03.000]  Okay.\n",
            "[06:03.000 --> 06:20.000]  That's it.\n",
            "[06:20.000 --> 06:35.000]  All I text you is.\n",
            "[06:35.000 --> 06:39.000]  Completes your whole day, doesn't it?\n",
            "[06:39.000 --> 06:49.000]  Hm.\n",
            "[06:49.000 --> 06:52.000]  I'm not wearing any underwear.\n",
            "[06:52.000 --> 06:57.000]  I can't see any blinds, can you?\n",
            "[06:57.000 --> 07:00.000]  Hm.\n",
            "[07:00.000 --> 07:08.000]  Hm.\n",
            "[07:08.000 --> 07:10.000]  That's it.\n",
            "[07:10.000 --> 07:11.000]  Stroke.\n",
            "[07:11.000 --> 07:13.000]  Stroke.\n",
            "[07:13.000 --> 07:17.000]  I'm going to give you the count of ten to come for me.\n",
            "[07:17.000 --> 07:19.000]  Are you ready?\n",
            "[07:19.000 --> 07:22.000]  Are you ready, my pet?\n",
            "[07:22.000 --> 07:23.000]  Good.\n",
            "[07:23.000 --> 07:24.000]  Ready.\n",
            "[07:24.000 --> 07:26.000]  Ten.\n",
            "[07:26.000 --> 07:27.000]  Nine.\n",
            "[07:27.000 --> 07:29.000]  Eight.\n",
            "[07:29.000 --> 07:30.000]  Seven.\n",
            "[07:30.000 --> 07:32.000]  Six.\n",
            "[07:32.000 --> 07:33.000]  Five.\n",
            "[07:33.000 --> 07:35.000]  Four.\n",
            "[07:35.000 --> 07:36.000]  Three.\n",
            "[07:36.000 --> 07:37.000]  Two.\n",
            "[07:37.000 --> 07:39.000]  One.\n",
            "[07:39.000 --> 07:40.000]  Come for me.\n",
            "[07:40.000 --> 07:41.000]  That's it.\n",
            "[07:41.000 --> 07:43.000]  Come for me.\n",
            "[07:43.000 --> 07:44.000]  Come for me.\n",
            "[07:44.000 --> 07:46.000]  Good boy.\n",
            "[07:46.000 --> 07:47.000]  Hm.\n",
            "[07:47.000 --> 07:48.000]  That's it.\n",
            "[07:48.000 --> 07:50.000]  Okay.\n",
            "[07:50.000 --> 07:51.000]  That's good.\n",
            "[07:51.000 --> 07:52.000]  Very good.\n",
            "[07:52.000 --> 07:55.000]  Okay.\n",
            "\n",
            "Processing: F:\\DATA\\Femdom\\AlexandraSnow\\More_Shiny_Tit_Worship-H265.m4v\n",
            "\n",
            "[00:00.000 --> 00:15.000]  ðŸŽµMUSICðŸŽµ\n",
            "[00:26.000 --> 00:27.000]  So you are my must.\n",
            "[00:30.000 --> 00:43.000]  These are my most popular videos.\n",
            "[00:43.000 --> 00:51.000]  Videos where you get to worship my amazing tits.\n",
            "[00:51.000 --> 00:55.000]  And I'll lead them up to my latex.\n",
            "[00:56.000 --> 00:59.000]  I happen to have this brand new outfit.\n",
            "[01:01.000 --> 01:04.000]  I'm going to let you stroke.\n",
            "[01:06.000 --> 01:12.000]  And imagine what it might be like to worship those tits.\n",
            "[01:17.000 --> 01:19.000]  Imagine what it might be like.\n",
            "[01:19.000 --> 01:27.000]  I love these sexy high-end outfits.\n",
            "[01:27.000 --> 01:34.000]  Especially when I get to unbutton them.\n",
            "[01:34.000 --> 01:37.000]  They're not quite yet.\n",
            "[01:49.000 --> 02:07.000]  Shiny my tits are. Shiny, glossy and black.\n",
            "[02:19.000 --> 02:24.000]  Yeah.\n",
            "[02:35.000 --> 02:39.000]  I bet you're waiting for me to start unpacking those buttons.\n",
            "[02:39.000 --> 02:48.000]  Not yet. I think I need a little more of this.\n",
            "[03:09.000 --> 03:20.000]  Shiny, slippery, wet.\n",
            "[03:39.000 --> 03:55.000]  Hmmm. There we go.\n",
            "[03:55.000 --> 04:12.000]  Is that enough cleavage?\n",
            "[04:12.000 --> 04:18.000]  Is that nice?\n",
            "[04:18.000 --> 04:22.000]  That slippery latex.\n",
            "[04:27.000 --> 04:29.000]  Perhaps a little more.\n",
            "[04:29.000 --> 04:32.000]  I think you can have a little more.\n",
            "[04:32.000 --> 04:35.000]  Yeah.\n",
            "[04:43.000 --> 04:46.000]  That's enough.\n",
            "[04:59.000 --> 05:02.000]  There we go.\n",
            "[05:02.000 --> 05:05.000]  Hmmm.\n",
            "[05:11.000 --> 05:16.000]  It's slippery. It's slippery tits.\n",
            "[05:16.000 --> 05:30.000]  So, it's thick. Shiny.\n",
            "[05:46.000 --> 05:59.000]  Hey there. Do you want to show those tits?\n",
            "[06:04.000 --> 06:07.000]  A little more.\n",
            "[06:16.000 --> 06:37.000]  Hmmm. Shiny.\n",
            "[06:46.000 --> 06:55.000]  Slippery. Wet.\n",
            "[07:02.000 --> 07:05.000]  Hmmm. There we go.\n",
            "[07:06.000 --> 07:12.000]  I'm enjoying that view.\n",
            "[07:12.000 --> 07:16.000]  Imagining I need to touch them.\n",
            "[07:16.000 --> 07:26.000]  I don't fucking think so.\n",
            "[07:26.000 --> 07:35.000]  Hmmm. No, you're just going to sit there and jerk. Jerk. Fantasize.\n",
            "[07:35.000 --> 07:43.000]  Hmmm. You want those tits so much.\n",
            "[07:43.000 --> 07:46.000]  Hmmm.\n",
            "[07:53.000 --> 08:02.000]  Enjoying that view? Slippery. Wet. Press.\n",
            "[08:02.000 --> 08:06.000]  How's that lube?\n",
            "[08:06.000 --> 08:10.000]  Oh, my lubes aren't even for you.\n",
            "[08:10.000 --> 08:16.000]  Hmmm. No, you forget to enjoy that.\n",
            "[08:25.000 --> 08:28.000]  I just love feeling it.\n",
            "[08:28.000 --> 08:35.000]  And teasing you. Making you work so hard for it.\n",
            "[08:35.000 --> 08:42.000]  Hmmm. You work so hard for my tits.\n",
            "[08:42.000 --> 08:49.000]  Okay, jerk. Jerk. Jerk for them.\n",
            "[08:49.000 --> 08:54.000]  Perfect there. Hmmm.\n",
            "[08:54.000 --> 09:01.000]  They're massive. And amazing. Hmmm.\n",
            "[09:06.000 --> 09:26.000]  So close. So close. So close.\n",
            "[09:26.000 --> 09:39.000]  Hmmm. Oh. Oh, he wants to see. He wants a little peek.\n",
            "[09:39.000 --> 09:45.000]  Hmmm. You don't get that yet.\n",
            "[09:45.000 --> 09:53.000]  Hmmm. Hmmm. You don't get that yet.\n",
            "[09:56.000 --> 10:05.000]  Poor boy. Poor boy.\n",
            "\n",
            "Processing: F:\\DATA\\Femdom\\AlexandraSnow\\Shiny_Tit_Tease-H265.m4v\n",
            "\n",
            "[00:00.000 --> 00:29.840]  Your obsession with my breasts has always landed\n",
            "[00:29.840 --> 00:33.480]  you in a little bit of trouble, hasn't it?\n",
            "[00:33.480 --> 00:47.120]  I mean, it seems like the moment you start to stare at them, your mind tends to go blank.\n",
            "[00:47.120 --> 01:00.560]  The cock gets hard, and you stroke, because you almost can't help yourself, can you?\n",
            "[01:00.560 --> 01:16.360]  Yeah, I understand, and that deep line of my cleavage makes it worse, doesn't it?\n",
            "[01:16.360 --> 01:25.760]  I get it.\n",
            "[01:25.760 --> 01:32.200]  But I want you to jerk for me.\n",
            "[01:32.200 --> 01:45.120]  I want you to be helpless and confrontive of my tits, because it's part of your biology.\n",
            "[01:45.120 --> 01:55.080]  Men can't help it, whether it's how you're programmed at birth, your obsession with your\n",
            "[01:55.080 --> 02:02.400]  mother's, obsession with being breastfed, or whether it just means some more deep in\n",
            "[02:02.400 --> 02:08.080]  your wiring that a woman with large breasts is fertile and powerful.\n",
            "[02:08.080 --> 02:15.300]  Well, it doesn't matter, don't overthink it.\n",
            "[02:15.300 --> 02:30.040]  The result is always the same, it's just these, these massive breasts.\n",
            "[02:30.040 --> 02:46.520]  So gorgeous, so stunning, and the more you jerk to them, the deeper and deeper your obsession\n",
            "[02:46.520 --> 02:51.520]  gets.\n",
            "[02:51.520 --> 03:05.480]  Better, not shoving them down in so much.\n",
            "[03:05.480 --> 03:18.480]  See those tits, they squeeze some, my nipples right there, really fit my tits in this thing.\n",
            "[03:18.520 --> 03:25.520]  There's so much of them.\n",
            "[03:25.520 --> 03:32.520]  Come on.\n",
            "[03:32.520 --> 03:39.520]  That's it.\n",
            "[03:39.520 --> 03:41.520]  There we go.\n",
            "[03:41.560 --> 03:49.560]  Stroke, stroke, stroke, stroke.\n",
            "[03:49.560 --> 04:00.560]  You know you want to, you know you want to, and you know the more that you try not to,\n",
            "[04:00.560 --> 04:04.560]  the harder it becomes.\n",
            "[04:04.560 --> 04:12.560]  Jerk, jerk, jerk to these tits, jerk to my tits.\n",
            "[04:12.560 --> 04:18.560]  Maybe I'll unzip it a little for you.\n",
            "[04:18.560 --> 04:20.560]  What do you think?\n",
            "[04:20.560 --> 04:25.560]  Think you can jerk a little harder, a little faster.\n",
            "[04:25.560 --> 04:32.560]  Maybe I'll give you a little peek.\n",
            "[04:32.560 --> 04:42.560]  There, is it better?\n",
            "[04:42.560 --> 04:51.560]  Come on, faster, jerk, jerk.\n",
            "[04:51.560 --> 04:56.560]  That's it, jerk, jerk, jerk.\n",
            "[04:56.560 --> 05:00.560]  That skin tight latex.\n",
            "[05:00.560 --> 05:03.560]  You want a different angle perhaps?\n",
            "[05:03.560 --> 05:06.560]  Here, come here, how's that?\n",
            "[05:06.560 --> 05:09.560]  Come on, jerk to those tits.\n",
            "[05:09.560 --> 05:21.560]  Jerk, jerk, jerk, and stroke, stroke.\n",
            "[05:21.560 --> 05:29.560]  There they are, so close to your face.\n",
            "[05:29.560 --> 05:35.560]  Aching, I know that cox is aching.\n",
            "[05:35.560 --> 05:37.560]  You want it too, don't you?\n",
            "[05:37.560 --> 05:44.560]  You want it.\n",
            "[05:44.560 --> 05:45.560]  That's it.\n",
            "[05:45.560 --> 05:53.560]  Stroke, stroke, stroke.\n",
            "[05:53.560 --> 06:07.560]  Faster, faster, faster, faster.\n",
            "[06:07.560 --> 06:09.560]  Good boy.\n",
            "[06:09.560 --> 06:15.560]  Stroke a little more, stroke a little more.\n",
            "[06:15.560 --> 06:19.560]  Good.\n",
            "[06:19.560 --> 06:21.560]  Keep going.\n",
            "[06:21.560 --> 06:25.560]  I'm going to give you ten seconds.\n",
            "[06:25.560 --> 06:29.560]  I'm going to give you ten seconds.\n",
            "[06:29.560 --> 06:32.560]  Are you ready?\n",
            "[06:32.560 --> 06:36.560]  Are you ready?\n",
            "[06:36.560 --> 06:51.560]  Ten, nine, eight, seven, six, five, four, three, two, one.\n",
            "[06:51.560 --> 06:53.560]  That's it, ready to come for my tits?\n",
            "[06:53.560 --> 06:55.560]  Are you ready to come for them?\n",
            "[06:55.560 --> 06:57.560]  Ready to come for that beautiful cleavage?\n",
            "[06:57.560 --> 06:58.560]  Ready to come?\n",
            "[06:58.560 --> 07:03.560]  All right, and come for me, come for me, come for me little slant.\n",
            "[07:03.560 --> 07:05.560]  That's it, come for me.\n",
            "[07:05.560 --> 07:10.560]  Good boy, good boy.\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess\n",
        "\n",
        "import whisper\n",
        "from whisper.utils import format_timestamp, get_writer, WriteTXT\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "import torch\n",
        "\n",
        "import math\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# select task\n",
        "\n",
        "task = \"Transcribe\" #@param [\"Transcribe\", \"Translate to English\"]\n",
        "\n",
        "task = \"transcribe\" if task == \"Transcribe\" else \"translate\"\n",
        "\n",
        "# select audio file\n",
        "\n",
        "\n",
        "audio_file = file_path_new #@param {type:\"string\"}\n",
        "\n",
        "audio_files = list(map(lambda audio_path: audio_path.strip(), audio_file.split(',')))\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  if not os.path.isfile(audio_path):\n",
        "    raise FileNotFoundError(audio_path)\n",
        "\n",
        "# set model\n",
        "\n",
        "use_model = \"medium\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\"]\n",
        "\n",
        "# select language\n",
        "\n",
        "language = \"English\" #@param [\"Auto-Detect\", \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Assamese\", \"Azerbaijani\", \"Bashkir\", \"Basque\", \"Belarusian\", \"Bengali\", \"Bosnian\", \"Breton\", \"Bulgarian\", \"Burmese\", \"Castilian\", \"Catalan\", \"Chinese\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Estonian\", \"Faroese\", \"Finnish\", \"Flemish\", \"French\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\", \"Hebrew\", \"Hindi\", \"Hungarian\", \"Icelandic\", \"Indonesian\", \"Italian\", \"Japanese\", \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Lao\", \"Latin\", \"Latvian\", \"Letzeburgesch\", \"Lingala\", \"Lithuanian\", \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Moldavian\", \"Moldovan\", \"Mongolian\", \"Myanmar\", \"Nepali\", \"Norwegian\", \"Nynorsk\", \"Occitan\", \"Panjabi\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Pushto\", \"Romanian\", \"Russian\", \"Sanskrit\", \"Serbian\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Sinhalese\", \"Slovak\", \"Slovenian\", \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tagalog\", \"Tajik\", \"Tamil\", \"Tatar\", \"Telugu\", \"Thai\", \"Tibetan\", \"Turkish\", \"Turkmen\", \"Ukrainian\", \"Urdu\", \"Uzbek\", \"Valencian\", \"Vietnamese\", \"Welsh\", \"Yiddish\", \"Yoruba\"]\n",
        "\n",
        "# other parameters\n",
        "\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "coherence_preference = \"More coherence, but may repeat text\" #@param [\"More coherence, but may repeat text\", \"Less repetitions, but may have less coherence\"]\n",
        "\n",
        "api_key = '' #@param {type:\"string\"}\n",
        "\n",
        "# detect device\n",
        "\n",
        "if api_key:\n",
        "  print(\"Using API\")\n",
        "\n",
        "  from pydub import AudioSegment\n",
        "  from pydub.silence import split_on_silence\n",
        "else:\n",
        "  DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  print(f\"Using {'GPU' if DEVICE == 'cuda' else 'CPU âš ï¸'}\")\n",
        "\n",
        "  # https://medium.com/analytics-vidhya/the-google-colab-system-specification-check-69d159597417\n",
        "  if DEVICE == \"cuda\":\n",
        "    !nvidia-smi -L\n",
        "  else:\n",
        "    if sys_platform == 'linux':\n",
        "      !lscpu | grep \"Model name\" | awk '{$1=$1};1'\n",
        "    \n",
        "    print(\"Not using GPU can result in a very slow execution\")\n",
        "    print(\"Ensure Hardware accelerator by GPU is enabled in Google Colab: Runtime > Change runtime type\")\n",
        "\n",
        "    if use_model not in ['tiny', 'base', 'small']:\n",
        "      print(\"You may also want to try a smaller model (tiny, base, small)\")\n",
        "\n",
        "# display language\n",
        "\n",
        "WHISPER_LANGUAGES = [k.title() for k in whisper.tokenizer.TO_LANGUAGE_CODE.keys()]\n",
        "\n",
        "if language == \"Auto-Detect\":\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\" and language not in WHISPER_LANGUAGES:\n",
        "  print(f\"\\nLanguage '{language}' is invalid\")\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\":\n",
        "  print(f\"\\nLanguage: {language}\")\n",
        "\n",
        "# load model\n",
        "\n",
        "if api_key:\n",
        "  print()\n",
        "else:\n",
        "  MODELS_WITH_ENGLISH_VERSION = [\"tiny\", \"base\", \"small\", \"medium\"]\n",
        "\n",
        "  if language == \"English\" and use_model in MODELS_WITH_ENGLISH_VERSION:\n",
        "    use_model += \".en\"\n",
        "\n",
        "  print(f\"\\nLoading {use_model} model... {os.path.expanduser(f'~/.cache/whisper/{use_model}.pt')}\")\n",
        "\n",
        "  model = whisper.load_model(use_model, device=DEVICE)\n",
        "\n",
        "  print(\n",
        "      f\"Model {use_model} is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "      f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,d} parameters.\\n\"\n",
        "  )\n",
        "\n",
        "# set options\n",
        "\n",
        "## https://github.com/openai/whisper/blob/v20231117/whisper/transcribe.py#L37\n",
        "## https://github.com/openai/whisper/blob/v20231117/whisper/decoding.py#L81\n",
        "options = {\n",
        "    'task': task,\n",
        "    'verbose': True,\n",
        "    'fp16': True,\n",
        "    'best_of': 5,\n",
        "    'beam_size': 5,\n",
        "    'patience': None,\n",
        "    'length_penalty': None,\n",
        "    'suppress_tokens': '-1',\n",
        "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0), # float or tuple\n",
        "    'condition_on_previous_text': coherence_preference == \"More coherence, but may repeat text\",\n",
        "    'initial_prompt': prompt or None,\n",
        "    'word_timestamps': False,\n",
        "}\n",
        "\n",
        "if api_key:\n",
        "  api_client = OpenAI(api_key=api_key)\n",
        "\n",
        "  api_supported_formats = ['mp3', 'mp4', 'mpeg', 'mpga', 'm4a', 'wav', 'webm']\n",
        "  api_max_bytes = 25 * 1024 * 1024 # 25 MB\n",
        "\n",
        "  api_transcribe = api_client.audio.transcriptions if task == 'transcribe' else api_client.audio.translations\n",
        "  api_transcribe = api_transcribe.create\n",
        "  \n",
        "  api_model = 'whisper-1' # large-v2\n",
        "\n",
        "  # https://platform.openai.com/docs/api-reference/audio?lang=python\n",
        "  api_options = {\n",
        "    'response_format': 'verbose_json',\n",
        "  }\n",
        "\n",
        "  if prompt:\n",
        "    api_options['prompt'] = prompt\n",
        "  \n",
        "  api_temperature = options['temperature'][0] if isinstance(options['temperature'], (tuple, list)) else options['temperature']\n",
        "  \n",
        "  if isinstance(api_temperature, (float, int)):\n",
        "    api_options['temperature'] = api_temperature\n",
        "  else:\n",
        "    raise ValueError(\"Invalid temperature type, it must be a float or a tuple of floats\")\n",
        "elif DEVICE == 'cpu':\n",
        "  options['fp16'] = False\n",
        "  torch.set_num_threads(os.cpu_count())\n",
        "\n",
        "# execute task\n",
        "# !whisper \"{audio_file}\" --task {task} --model {use_model} --output_dir {output_dir} --device {DEVICE} --verbose {options['verbose']}\n",
        "\n",
        "if task == \"translate\":\n",
        "  print(\"-- TRANSLATE TO ENGLISH --\")\n",
        "else:\n",
        "  print(\"-- TRANSCRIPTION --\")\n",
        "\n",
        "results = {} # audio_path to result\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  print(f\"\\nProcessing: {audio_path}\\n\")\n",
        "\n",
        "  # detect language\n",
        "  detect_language = not language or language == \"detect\"\n",
        "\n",
        "  if not detect_language:\n",
        "    options['language'] = language\n",
        "    source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(language.lower())\n",
        "  elif not api_key:\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    source_language_code = max(probs, key=probs.get)\n",
        "    options['language'] = whisper.tokenizer.LANGUAGES[source_language_code].title()\n",
        "    \n",
        "    print(f\"Detected language: {options['language']}\\n\")\n",
        "\n",
        "  # transcribe\n",
        "  if api_key:\n",
        "    # API\n",
        "    if task == \"transcribe\" and not detect_language:\n",
        "      api_options['language'] = source_language_code\n",
        "    \n",
        "    source_audio_name_path, source_audio_ext = os.path.splitext(audio_path)\n",
        "    source_audio_ext = source_audio_ext[1:]\n",
        "\n",
        "    if source_audio_ext in api_supported_formats:\n",
        "      api_audio_path = audio_path\n",
        "      api_audio_ext = source_audio_ext\n",
        "    else:\n",
        "      ## convert audio file to a supported format\n",
        "      if options['verbose']:\n",
        "        print(f\"API supported formats: {','.join(api_supported_formats)}\")\n",
        "        print(f\"Converting {source_audio_ext} audio to a supported format...\")\n",
        "\n",
        "      api_audio_ext = 'mp3'\n",
        "\n",
        "      api_audio_path = f'{source_audio_name_path}.{api_audio_ext}'\n",
        "\n",
        "      subprocess.run(['ffmpeg', '-i', audio_path, api_audio_path], check=True, capture_output=True)\n",
        "\n",
        "      if options['verbose']:\n",
        "        print(api_audio_path, end='\\n\\n')\n",
        "\n",
        "    ## split audio file in chunks\n",
        "    api_audio_chunks = []\n",
        "\n",
        "    audio_bytes = os.path.getsize(api_audio_path)\n",
        "\n",
        "    if audio_bytes >= api_max_bytes:\n",
        "      if options['verbose']:\n",
        "        print(f\"Audio exceeds API maximum allowed file size.\\nSplitting audio in chunks...\")\n",
        "      \n",
        "      audio_segment_file = AudioSegment.from_file(api_audio_path, api_audio_ext)\n",
        "\n",
        "      min_chunks = math.ceil(audio_bytes / (api_max_bytes / 2))\n",
        "\n",
        "      # print(f\"Min chunks: {min_chunks}\")\n",
        "\n",
        "      max_chunk_milliseconds = int(len(audio_segment_file) // min_chunks)\n",
        "\n",
        "      # print(f\"Max chunk milliseconds: {max_chunk_milliseconds}\")\n",
        "\n",
        "      def add_chunk(api_audio_chunk):\n",
        "        api_audio_chunk_path = f\"{source_audio_name_path}_{len(api_audio_chunks) + 1}.{api_audio_ext}\"\n",
        "        api_audio_chunk.export(api_audio_chunk_path, format=api_audio_ext)\n",
        "        api_audio_chunks.append(api_audio_chunk_path)\n",
        "      \n",
        "      def raw_split(big_chunk):\n",
        "        subchunks = math.ceil(len(big_chunk) / max_chunk_milliseconds)\n",
        "\n",
        "        for subchunk_i in range(subchunks):\n",
        "          chunk_start = max_chunk_milliseconds * subchunk_i\n",
        "          chunk_end = min(max_chunk_milliseconds * (subchunk_i + 1), len(big_chunk))\n",
        "          add_chunk(big_chunk[chunk_start:chunk_end])\n",
        "      \n",
        "      non_silent_chunks = split_on_silence(audio_segment_file,\n",
        "                                           seek_step=5, # ms\n",
        "                                           min_silence_len=1250, # ms\n",
        "                                           silence_thresh=-25, # dB\n",
        "                                           keep_silence=True) # needed to aggregate timestamps\n",
        "\n",
        "      # print(f\"Non silent chunks: {len(non_silent_chunks)}\")\n",
        "      \n",
        "      current_chunk = non_silent_chunks[0] if non_silent_chunks else audio_segment_file\n",
        "\n",
        "      for next_chunk in non_silent_chunks[1:]:\n",
        "        if len(current_chunk) > max_chunk_milliseconds:\n",
        "          raw_split(current_chunk)\n",
        "          current_chunk = next_chunk\n",
        "        elif len(current_chunk) + len(next_chunk) <= max_chunk_milliseconds:\n",
        "          current_chunk += next_chunk\n",
        "        else:\n",
        "          add_chunk(current_chunk)\n",
        "          current_chunk = next_chunk\n",
        "      \n",
        "      if len(current_chunk) > max_chunk_milliseconds:\n",
        "        raw_split(current_chunk)\n",
        "      else:\n",
        "        add_chunk(current_chunk)\n",
        "      \n",
        "      if options['verbose']:\n",
        "        print(f'Total chunks: {len(api_audio_chunks)}\\n')\n",
        "    else:\n",
        "      api_audio_chunks.append(api_audio_path)\n",
        "    \n",
        "    ## process chunks\n",
        "    result = None\n",
        "\n",
        "    for api_audio_chunk_path in api_audio_chunks:\n",
        "      ## API request\n",
        "      with open(api_audio_chunk_path, 'rb') as api_audio_file:\n",
        "        api_result = api_transcribe(model=api_model, file=api_audio_file, **api_options)\n",
        "        api_result = api_result.model_dump() # to dict\n",
        "      \n",
        "      api_segments = api_result['segments']\n",
        "      \n",
        "      if result:\n",
        "        ## update timestamps\n",
        "        last_segment_timestamp = result['segments'][-1]['end'] if result['segments'] else 0\n",
        "\n",
        "        for segment in api_segments:\n",
        "          segment['start'] += last_segment_timestamp\n",
        "          segment['end'] += last_segment_timestamp\n",
        "\n",
        "        ## append new segments\n",
        "        result['segments'].extend(api_segments)\n",
        "        \n",
        "        if 'duration' in result:\n",
        "          result['duration'] += api_result.get('duration', 0)\n",
        "      else:\n",
        "        ## first request\n",
        "        result = api_result\n",
        "        \n",
        "        if detect_language:\n",
        "          print(f\"Detected language: {result['language'].title()}\\n\")\n",
        "    \n",
        "      ## display segments\n",
        "      if options['verbose']:\n",
        "        for segment in api_segments:\n",
        "          print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {segment['text']}\")\n",
        "  else:\n",
        "    # Open-Source\n",
        "    result = whisper.transcribe(model, audio_path, **options)\n",
        "\n",
        "  # fix results formatting\n",
        "  for segment in result['segments']:\n",
        "    segment['text'] = segment['text'].strip()\n",
        "  \n",
        "  result['text'] = '\\n'.join(map(lambda segment: segment['text'], result['segments']))\n",
        "\n",
        "  # set results for this audio file\n",
        "  results[audio_path] = result\n",
        "  #os.rename(file_path_new, file_path) # restore original file name"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrxbUivk_h3"
      },
      "source": [
        "## [Step 4] ðŸ’¾ **Save results**\n",
        "\n",
        "Run this cell to write the transcription as a file output.\n",
        "\n",
        "Results will be available in the **audio_transcription** folder in the formats selected in `output_formats`.\n",
        "\n",
        "If you don't see that folder, you may need to refresh ðŸ”„ the Files folder.\n",
        "\n",
        "Available formats: `txt,vtt,srt,tsv,json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "wNsrB45_lCIl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing results...\n",
            "\n",
            "F:\\DATA\\Femdom\\Black_Latex_Joi-H265.srt\n",
            "\n",
            "F:\\DATA\\Femdom\\More_Shiny_Tit_Worship-H265.srt\n",
            "\n",
            "F:\\DATA\\Femdom\\Shiny_Tit_Tease-H265.srt\n"
          ]
        }
      ],
      "source": [
        "# set output folder\n",
        "from pathlib import Path\n",
        "# output_dir = \"audio_transcription\"\n",
        "output_dir = os.path.dirname(file_path_list[0])\n",
        "\n",
        "# set output formats: https://github.com/openai/whisper/blob/v20231117/whisper/utils.py#L283\n",
        "output_formats = \"srt\" #@param [\"txt,vtt,srt,tsv,json\", \"txt,vtt,srt\", \"txt,vtt\", \"txt,srt\", \"txt\", \"vtt\", \"srt\", \"tsv\", \"json\"] {allow-input: true}\n",
        "output_formats = output_formats.split(',')\n",
        "\n",
        "from typing import TextIO\n",
        "\n",
        "class WriteText(WriteTXT):\n",
        "\n",
        "  def write_result(self, result: dict, file: TextIO, **kwargs):\n",
        "    print(result['text'], file=file, flush=True)\n",
        "\n",
        "def write_result(result, output_format, output_file_name):\n",
        "  output_format = output_format.strip()\n",
        "\n",
        "  # start captions in non-zero timestamp (some media players does not detect the first caption)\n",
        "  fix_vtt = output_format == 'vtt' and result['segments'] and result['segments'][0].get('start') == 0\n",
        "  \n",
        "  if fix_vtt:\n",
        "    result['segments'][0]['start'] += 1/1000 # +1ms\n",
        "\n",
        "  # write result in the desired format\n",
        "  writer = WriteText(output_dir) if output_format == 'txt' else get_writer(output_format, output_dir)\n",
        "  writer(result, output_file_name)\n",
        "\n",
        "  if fix_vtt:\n",
        "    result['segments'][0]['start'] = 0 # reset change\n",
        "\n",
        "  output_file_path = os.path.join(output_dir, f\"{output_file_name}.{output_format}\")\n",
        "  print(output_file_path)\n",
        "\n",
        "# save results\n",
        "\n",
        "print(\"Writing results...\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for audio_path, result in results.items():\n",
        "  print(end='\\n')\n",
        "  \n",
        "  # output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "  output_file_name = Path(audio_path).stem.replace('.', '_') # remove spaces to make life easier\n",
        "  for output_format in output_formats:\n",
        "    write_result(result, output_format, output_file_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfkDhNMMvY8s"
      },
      "source": [
        "## [Step 5] ðŸ’¬ Translate results with DeepL (API key needed)\n",
        "\n",
        "This is an **optional** step to translate the transcription to another language using the **DeepL** API.\n",
        "\n",
        "[Get a DeepL Developer Account API Key](https://www.deepl.com/pro-api)\n",
        "\n",
        "Set the `deepl_api_key` to translate the transcription to a supported language in `deepl_target_language`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "28f7EIP-rez0"
      },
      "outputs": [],
      "source": [
        "import deepl\n",
        "\n",
        "# translation service options (DeepL Developer Account)\n",
        "\n",
        "deepl_api_key = \"\" #@param {type:\"string\"}\n",
        "\n",
        "deepl_target_language = \"\" #@param [\"\", \"Bulgarian\", \"Chinese (simplified)\", \"Czech\", \"Danish\", \"Dutch\", \"English (American)\", \"English (British)\", \"Estonian\", \"Finnish\", \"French\", \"German\", \"Greek\", \"Hungarian\", \"Indonesian\", \"Italian\", \"Japanese\", \"Korean\", \"Latvian\", \"Lithuanian\", \"Norwegian\", \"Polish\", \"Portuguese (Brazilian)\", \"Portuguese (European)\", \"Romanian\", \"Russian\", \"Slovak\", \"Slovenian\", \"Spanish\", \"Swedish\", \"Turkish\", \"Ukrainian\"]\n",
        "\n",
        "deepl_formality = \"default\" #@param [\"default\", \"formal\", \"informal\"]\n",
        "\n",
        "deepl_coherence_preference = \"Share context between lines\" #@param [\"Share context between lines\", \"Translate each line independently\"]\n",
        "deepl_coherence_preference = deepl_coherence_preference == \"Share context between lines\"\n",
        "\n",
        "if not deepl_api_key:\n",
        "  print(\"Required: deepl_api_key\")\n",
        "  print(\"Get a DeepL Developer Account API Key: https://www.deepl.com/pro-api\")\n",
        "\n",
        "if not deepl_target_language:\n",
        "  print(\"Required: deepl_target_language\")\n",
        "elif deepl_target_language == 'English':\n",
        "  deepl_target_language = \"English (British)\"\n",
        "elif deepl_target_language == 'Chinese':\n",
        "  deepl_target_language = \"Chinese (simplified)\"\n",
        "elif deepl_target_language == 'Portuguese':\n",
        "  deepl_target_language = \"Portuguese (European)\"\n",
        "\n",
        "use_deepl_translation = deepl_api_key and deepl_target_language\n",
        "\n",
        "if use_deepl_translation:\n",
        "  if deepl_formality != 'default':\n",
        "    deepl_formality = 'prefer_more' if deepl_formality == 'formal' else 'prefer_less'\n",
        "\n",
        "  translated_results = {} # audio_path to translated results\n",
        "\n",
        "  try:\n",
        "    deepl_translator = deepl.Translator(deepl_api_key)\n",
        "\n",
        "    deepl_source_languages = [lang.code.upper() for lang in deepl_translator.get_source_languages()]\n",
        "    \n",
        "    deepl_target_languages_dict = deepl_translator.get_target_languages()\n",
        "    deepl_target_languages = [lang.name for lang in deepl_target_languages_dict]\n",
        "\n",
        "    deepl_target_language_code = next(lang.code for lang in deepl_target_languages_dict if lang.name == deepl_target_language).upper()\n",
        "    target_language_code = deepl_target_language_code.split('-')[0]\n",
        "    \n",
        "    for audio_path, result in results.items():\n",
        "      deepl_usage = deepl_translator.get_usage()\n",
        "      \n",
        "      if deepl_usage.any_limit_reached:\n",
        "        print(audio_path)\n",
        "        raise deepl.DeepLException(\"Quota for this billing period has been exceeded, message: Quota Exceeded\")\n",
        "      else:\n",
        "        print(audio_path + '\\n')\n",
        "      \n",
        "      # translate results (DeepL)\n",
        "      source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(result['language'].lower()).upper()\n",
        "\n",
        "      if (task == 'translate' and target_language_code != 'EN') or (task == 'transcribe' and source_language_code in deepl_source_languages and source_language_code != target_language_code):\n",
        "        source_lang = source_language_code if task == 'transcribe' else None\n",
        "        translate_from = f\"from {result['language'].title()} [{source_language_code}] \" if source_lang else ''\n",
        "        print(f\"DeepL: Translate results {translate_from}to {deepl_target_language} [{deepl_target_language_code}]\\n\")\n",
        "\n",
        "        segments = result['segments']\n",
        "\n",
        "        translated_results[audio_path] = { 'text': '', 'segments': [], 'language': deepl_target_language }\n",
        "\n",
        "        # segments / request (max 128 KiB / request, so deepl_batch_requests_size is limited to around 1000)\n",
        "        deepl_batch_requests_size = 200 # 200 segments * ~100 bytes / segment = ~20 KB / request  (~15 minutes of speech)\n",
        "        \n",
        "        for batch_segments in [segments[i:i + deepl_batch_requests_size] for i in range(0, len(segments), deepl_batch_requests_size)]:\n",
        "          batch_segments_text = [segment['text'] for segment in batch_segments]\n",
        "\n",
        "          if deepl_coherence_preference:\n",
        "            batch_segments_text = '<br/>'.join(batch_segments_text)\n",
        "\n",
        "          # DeepL request\n",
        "          deepl_results = deepl_translator.translate_text(\n",
        "              text=batch_segments_text,\n",
        "              source_lang=source_lang,\n",
        "              target_lang=deepl_target_language_code,\n",
        "              formality=deepl_formality,\n",
        "              split_sentences='nonewlines',\n",
        "              tag_handling='xml' if deepl_coherence_preference else None,\n",
        "              ignore_tags='br' if deepl_coherence_preference else None, # used to synchronize sentences with whisper lines but without splitting sentences in DeepL\n",
        "              outline_detection=False if deepl_coherence_preference else None\n",
        "          )\n",
        "          \n",
        "          deepl_results_segments = deepl_results.text.split('<br/>') if deepl_coherence_preference else [deepl_result_segment.text for deepl_result_segment in deepl_results]\n",
        "\n",
        "          for j, translated_text in enumerate(deepl_results_segments):\n",
        "            segment = batch_segments[j]\n",
        "\n",
        "            # fix sentence formatting\n",
        "            translated_text = translated_text.lstrip(',.ã€‚ ').rstrip()\n",
        "\n",
        "            if not deepl_coherence_preference and translated_text and translated_text[-1] in '.ã€‚' and segment['text'][-1] not in '.ã€‚':\n",
        "              translated_text = translated_text[:-1]\n",
        "\n",
        "            # add translated segments\n",
        "            translated_results[audio_path]['segments'].append(dict(id=segment['id'], start=segment['start'], end=segment['end'], text=translated_text))\n",
        "\n",
        "            if options['verbose']:\n",
        "              print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {translated_text}\")\n",
        "        \n",
        "        deepl_usage = deepl_translator.get_usage()\n",
        "        \n",
        "        if deepl_usage.character.valid:\n",
        "          print(f\"\\nDeepL: Character usage: {deepl_usage.character.count} / {deepl_usage.character.limit} ({100*(deepl_usage.character.count/deepl_usage.character.limit):.2f}%)\\n\")\n",
        "      elif source_language_code == target_language_code:\n",
        "        print(f\"Nothing to translate. Results are already in {result['language']}.\")\n",
        "      elif task == 'transcribe' and source_language_code not in deepl_source_languages:\n",
        "        print(f\"DeepL: {result['language']} is not yet supported\")\n",
        "  except deepl.DeepLException as e:\n",
        "    if isinstance(e, deepl.AuthorizationException) and str(e) == \"Authorization failure, check auth_key\":\n",
        "      e = \"Authorization failure, check deepl_api_key\"\n",
        "    print(f\"\\nDeepL: [Error] {e}\\n\")\n",
        "  \n",
        "  # save translated results (if any)\n",
        "\n",
        "  if translated_results:\n",
        "    print(\"Writing translated results...\")\n",
        "\n",
        "    for audio_path, translated_result in translated_results.items():\n",
        "      print(end='\\n')\n",
        "\n",
        "      translated_result['text'] = '\\n'.join(map(lambda translated_segment: translated_segment['text'], translated_result['segments']))\n",
        "      \n",
        "      output_file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "      translated_output_file_name = f\"{output_file_name}_{deepl_target_language}\"\n",
        "\n",
        "      for output_format in output_formats:\n",
        "        write_result(translated_result, output_format, translated_output_file_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "cf5167ffd3d40c187bbdee173a0e169759f2b54f4182487e61a0f59820dcd535"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
