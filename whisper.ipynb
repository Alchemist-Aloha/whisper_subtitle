{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load media files separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This cell define the input file path\n",
    "file_path = [\"F:\\dir\\\\video.mp4\", \"F:\\dir\\\\video2.mp4\"]\n",
    "if type(file_path) == str:\n",
    "    print('File_path is a string')\n",
    "    # remove spaces to make life easier\n",
    "    file_path_new = file_path.replace(' ', '_')\n",
    "    file_path_new = file_path_new.replace('(', '_')\n",
    "    file_path_new = file_path_new.replace(')', '_')\n",
    "    file_path_new = file_path_new.replace(\n",
    "        ',', '')  # remove commas to avoid issues\n",
    "    os.rename(file_path, file_path_new)  # rename file to avoid issues\n",
    "else:\n",
    "    file_path_list = file_path.copy()\n",
    "    file_path_new = ''\n",
    "    for i in range(len(file_path)):\n",
    "        file_path_list[i] = file_path[i].replace(' ', '_')\n",
    "        file_path_list[i] = file_path_list[i].replace('(', '_')\n",
    "        file_path_list[i] = file_path_list[i].replace(')', '_')\n",
    "        file_path_list[i] = file_path_list[i].replace(\n",
    "            ',', '')  # remove commas to avoid issues\n",
    "        # rename file to avoid issues\n",
    "        os.rename(file_path[i], file_path_list[i])\n",
    "    audio_file = \",\".join(file_path_list)\n",
    "print(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load media directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_m4v_file_paths(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        # Construct the new filename by replacing spaces, parentheses, and commas\n",
    "        new_filename = filename.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\").replace(\",\", \"_\").replace(\".\", \"_\")\n",
    "        new_filename = filename.replace(\"_m4v\" , \".m4v\").replace(\"_srt\" , \".srt\")\n",
    "        # Get the full paths for the old and new filenames\n",
    "        old_file = os.path.join(directory, filename)\n",
    "        new_file = os.path.join(directory, new_filename)\n",
    "        # Rename the file\n",
    "        if old_file != new_file:\n",
    "            os.rename(old_file, new_file)\n",
    "    # List all .m4v files in the specified directory with full paths\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\n",
    "        '.m4v') and os.path.isfile(os.path.join(directory, f))]\n",
    "    # Join the file paths into a single string separated by commas\n",
    "    file_paths = \",\".join(files)\n",
    "    return file_paths\n",
    "\n",
    "# This cell define the input file path\n",
    "audio_file = get_m4v_file_paths(\"E:\\dir\")\n",
    "print(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start transcribing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "from whisper.utils import format_timestamp, get_writer, WriteTXT\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from openai import OpenAI\n",
    "from typing import TextIO\n",
    "\n",
    "\n",
    "# select task\n",
    "task = \"translate\" #@param [\"Transcribe\", \"Translate to English\"]\n",
    "\n",
    "# set output formats: https://github.com/openai/whisper/blob/v20231117/whisper/utils.py#L283\n",
    "output_formats = \"srt\" #@param [\"txt,vtt,srt,tsv,json\", \"txt,vtt,srt\", \"txt,vtt\", \"txt,srt\", \"txt\", \"vtt\", \"srt\", \"tsv\", \"json\"] {allow-input: true}\n",
    "output_formats = output_formats.split(',')\n",
    "\n",
    "# set model\n",
    "\n",
    "use_model = \"medium\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\", \"turbo\"]\n",
    "\n",
    "# select language\n",
    "\n",
    "language = \"German\" #@param [\"Auto-Detect\", \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Assamese\", \"Azerbaijani\", \"Bashkir\", \"Basque\", \"Belarusian\", \"Bengali\", \"Bosnian\", \"Breton\", \"Bulgarian\", \"Burmese\", \"Castilian\", \"Catalan\", \"Chinese\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Estonian\", \"Faroese\", \"Finnish\", \"Flemish\", \"French\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\", \"Hebrew\", \"Hindi\", \"Hungarian\", \"Icelandic\", \"Indonesian\", \"Italian\", \"Japanese\", \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Lao\", \"Latin\", \"Latvian\", \"Letzeburgesch\", \"Lingala\", \"Lithuanian\", \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Moldavian\", \"Moldovan\", \"Mongolian\", \"Myanmar\", \"Nepali\", \"Norwegian\", \"Nynorsk\", \"Occitan\", \"Panjabi\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Pushto\", \"Romanian\", \"Russian\", \"Sanskrit\", \"Serbian\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Sinhalese\", \"Slovak\", \"Slovenian\", \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tagalog\", \"Tajik\", \"Tamil\", \"Tatar\", \"Telugu\", \"Thai\", \"Tibetan\", \"Turkish\", \"Turkmen\", \"Ukrainian\", \"Urdu\", \"Uzbek\", \"Valencian\", \"Vietnamese\", \"Welsh\", \"Yiddish\", \"Yoruba\"]\n",
    "\n",
    "# other parameters\n",
    "\n",
    "prompt = \"\" #@param {type:\"string\"}\n",
    "\n",
    "coherence_preference = \"Less repetitions, but may have less coherence\" #@param [\"More coherence, but may repeat text\", \"Less repetitions, but may have less coherence\"]\n",
    "\n",
    "api_key = '' #@param {type:\"string\"}\n",
    "\n",
    "# select audio file\n",
    "# audio_file = file_path_new #@param {type:\"string\"}\n",
    "audio_files = list(map(lambda audio_path: audio_path.strip(), audio_file.split(',')))\n",
    "\n",
    "for audio_path in audio_files:\n",
    "  if not os.path.isfile(audio_path):\n",
    "    raise FileNotFoundError(audio_path)\n",
    "\n",
    "# write result to file\n",
    "class WriteText(WriteTXT):\n",
    "\n",
    "  def write_result(self, result: dict, file: TextIO, **kwargs):\n",
    "    print(result['text'], file=file, flush=True)\n",
    "# write result to file\n",
    "def write_result(result, output_format, output_file_name):\n",
    "  output_format = output_format.strip()\n",
    "\n",
    "  # start captions in non-zero timestamp (some media players does not detect the first caption)\n",
    "  fix_vtt = output_format == 'vtt' and result['segments'] and result['segments'][0].get('start') == 0\n",
    "  \n",
    "  if fix_vtt:\n",
    "    result['segments'][0]['start'] += 1/1000 # +1ms\n",
    "\n",
    "  # write result in the desired format\n",
    "  writer = WriteText(output_dir) if output_format == 'txt' else get_writer(output_format, output_dir)\n",
    "  writer(result, output_file_name)\n",
    "\n",
    "  if fix_vtt:\n",
    "    result['segments'][0]['start'] = 0 # reset change\n",
    "\n",
    "  output_file_path = os.path.join(output_dir, f\"{output_file_name}.{output_format}\")\n",
    "  print(output_file_path)  \n",
    "  \n",
    "# detect device\n",
    "\n",
    "if api_key:\n",
    "  print(\"Using API\")\n",
    "\n",
    "  from pydub import AudioSegment\n",
    "  from pydub.silence import split_on_silence\n",
    "else:\n",
    "  DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  print(f\"Using {'GPU' if DEVICE == 'cuda' else 'CPU ⚠️'}\")\n",
    "\n",
    "  # https://medium.com/analytics-vidhya/the-google-colab-system-specification-check-69d159597417\n",
    "  if DEVICE == \"cuda\":\n",
    "    !nvidia-smi -L\n",
    "  else:\n",
    "    if sys.platform == 'linux':\n",
    "      !lscpu | grep \"Model name\" | awk '{$1=$1};1'\n",
    "    \n",
    "    print(\"Not using GPU can result in a very slow execution\")\n",
    "    print(\"Ensure Hardware accelerator by GPU is enabled in Google Colab: Runtime > Change runtime type\")\n",
    "\n",
    "    if use_model not in ['tiny', 'base', 'small']:\n",
    "      print(\"You may also want to try a smaller model (tiny, base, small)\")\n",
    "\n",
    "# display language\n",
    "\n",
    "WHISPER_LANGUAGES = [k.title() for k in whisper.tokenizer.TO_LANGUAGE_CODE.keys()]\n",
    "\n",
    "if language == \"Auto-Detect\":\n",
    "  language = \"detect\"\n",
    "\n",
    "if language and language != \"detect\" and language not in WHISPER_LANGUAGES:\n",
    "  print(f\"\\nLanguage '{language}' is invalid\")\n",
    "  language = \"detect\"\n",
    "\n",
    "if language and language != \"detect\":\n",
    "  print(f\"\\nLanguage: {language}\")\n",
    "\n",
    "# load model\n",
    "\n",
    "if api_key:\n",
    "  print()\n",
    "else:\n",
    "  MODELS_WITH_ENGLISH_VERSION = [\"tiny\", \"base\", \"small\", \"medium\"]\n",
    "\n",
    "  if language == \"English\" and use_model in MODELS_WITH_ENGLISH_VERSION:\n",
    "    use_model += \".en\"\n",
    "\n",
    "  print(f\"\\nLoading {use_model} model... {os.path.expanduser(f'~/.cache/whisper/{use_model}.pt')}\")\n",
    "\n",
    "  model = whisper.load_model(use_model, device=DEVICE)\n",
    "\n",
    "  print(\n",
    "      f\"Model {use_model} is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "      f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,d} parameters.\\n\"\n",
    "  )\n",
    "\n",
    "# set options\n",
    "\n",
    "## https://github.com/openai/whisper/blob/v20231117/whisper/transcribe.py#L37\n",
    "## https://github.com/openai/whisper/blob/v20231117/whisper/decoding.py#L81\n",
    "options = {\n",
    "    'task': task,\n",
    "    'verbose': True,\n",
    "    'fp16': True,\n",
    "    'best_of': 5,\n",
    "    'beam_size': 5,\n",
    "    'patience': None,\n",
    "    'length_penalty': None,\n",
    "    'suppress_tokens': '-1',\n",
    "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0), # float or tuple\n",
    "    'condition_on_previous_text': False,\n",
    "    'initial_prompt': prompt or None,\n",
    "    'word_timestamps': False,\n",
    "}\n",
    "\n",
    "if api_key:\n",
    "  api_client = OpenAI(api_key=api_key)\n",
    "\n",
    "  api_supported_formats = ['mp3', 'mp4', 'mpeg', 'mpga', 'm4a', 'wav', 'webm']\n",
    "  api_max_bytes = 25 * 1024 * 1024 # 25 MB\n",
    "\n",
    "  api_transcribe = api_client.audio.transcriptions if task == 'transcribe' else api_client.audio.translations\n",
    "  api_transcribe = api_transcribe.create\n",
    "  \n",
    "  api_model = 'whisper-1' # large-v2\n",
    "\n",
    "  # https://platform.openai.com/docs/api-reference/audio?lang=python\n",
    "  api_options = {\n",
    "    'response_format': 'verbose_json',\n",
    "  }\n",
    "\n",
    "  if prompt:\n",
    "    api_options['prompt'] = prompt\n",
    "  \n",
    "  api_temperature = options['temperature'][0] if isinstance(options['temperature'], (tuple, list)) else options['temperature']\n",
    "  \n",
    "  if isinstance(api_temperature, (float, int)):\n",
    "    api_options['temperature'] = api_temperature\n",
    "  else:\n",
    "    raise ValueError(\"Invalid temperature type, it must be a float or a tuple of floats\")\n",
    "elif DEVICE == 'cpu':\n",
    "  options['fp16'] = False\n",
    "  torch.set_num_threads(os.cpu_count())\n",
    "\n",
    "# execute task\n",
    "# !whisper \"{audio_file}\" --task {task} --model {use_model} --output_dir {output_dir} --device {DEVICE} --verbose {options['verbose']}\n",
    "\n",
    "if task == \"translate\":\n",
    "  print(\"-- TRANSLATE TO ENGLISH --\")\n",
    "else:\n",
    "  print(\"-- TRANSCRIPTION --\")\n",
    "\n",
    "results = {} # audio_path to result\n",
    "\n",
    "for audio_path in audio_files:\n",
    "  print(f\"\\nProcessing: {audio_path}\\n\")\n",
    "\n",
    "  # detect language\n",
    "  detect_language = not language or language == \"detect\"\n",
    "\n",
    "  if not detect_language:\n",
    "    options['language'] = language\n",
    "    source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(language.lower())\n",
    "  elif not api_key:\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "\n",
    "    source_language_code = max(probs, key=probs.get)\n",
    "    options['language'] = whisper.tokenizer.LANGUAGES[source_language_code].title()\n",
    "    \n",
    "    print(f\"Detected language: {options['language']}\\n\")\n",
    "\n",
    "  # transcribe\n",
    "  if api_key:\n",
    "    # API\n",
    "    if task == \"transcribe\" and not detect_language:\n",
    "      api_options['language'] = source_language_code\n",
    "    \n",
    "    source_audio_name_path, source_audio_ext = os.path.splitext(audio_path)\n",
    "    source_audio_ext = source_audio_ext[1:]\n",
    "\n",
    "    if source_audio_ext in api_supported_formats:\n",
    "      api_audio_path = audio_path\n",
    "      api_audio_ext = source_audio_ext\n",
    "    else:\n",
    "      ## convert audio file to a supported format\n",
    "      if options['verbose']:\n",
    "        print(f\"API supported formats: {','.join(api_supported_formats)}\")\n",
    "        print(f\"Converting {source_audio_ext} audio to a supported format...\")\n",
    "\n",
    "      api_audio_ext = 'mp3'\n",
    "\n",
    "      api_audio_path = f'{source_audio_name_path}.{api_audio_ext}'\n",
    "\n",
    "      subprocess.run(['ffmpeg', '-i', audio_path, api_audio_path], check=True, capture_output=True)\n",
    "\n",
    "      if options['verbose']:\n",
    "        print(api_audio_path, end='\\n\\n')\n",
    "\n",
    "    ## split audio file in chunks\n",
    "    api_audio_chunks = []\n",
    "\n",
    "    audio_bytes = os.path.getsize(api_audio_path)\n",
    "\n",
    "    if audio_bytes >= api_max_bytes:\n",
    "      if options['verbose']:\n",
    "        print(f\"Audio exceeds API maximum allowed file size.\\nSplitting audio in chunks...\")\n",
    "      \n",
    "      audio_segment_file = AudioSegment.from_file(api_audio_path, api_audio_ext)\n",
    "\n",
    "      min_chunks = math.ceil(audio_bytes / (api_max_bytes / 2))\n",
    "\n",
    "      # print(f\"Min chunks: {min_chunks}\")\n",
    "\n",
    "      max_chunk_milliseconds = int(len(audio_segment_file) // min_chunks)\n",
    "\n",
    "      # print(f\"Max chunk milliseconds: {max_chunk_milliseconds}\")\n",
    "\n",
    "      def add_chunk(api_audio_chunk):\n",
    "        api_audio_chunk_path = f\"{source_audio_name_path}_{len(api_audio_chunks) + 1}.{api_audio_ext}\"\n",
    "        api_audio_chunk.export(api_audio_chunk_path, format=api_audio_ext)\n",
    "        api_audio_chunks.append(api_audio_chunk_path)\n",
    "      \n",
    "      def raw_split(big_chunk):\n",
    "        subchunks = math.ceil(len(big_chunk) / max_chunk_milliseconds)\n",
    "\n",
    "        for subchunk_i in range(subchunks):\n",
    "          chunk_start = max_chunk_milliseconds * subchunk_i\n",
    "          chunk_end = min(max_chunk_milliseconds * (subchunk_i + 1), len(big_chunk))\n",
    "          add_chunk(big_chunk[chunk_start:chunk_end])\n",
    "      \n",
    "      non_silent_chunks = split_on_silence(audio_segment_file,\n",
    "                                           seek_step=5, # ms\n",
    "                                           min_silence_len=1250, # ms\n",
    "                                           silence_thresh=-25, # dB\n",
    "                                           keep_silence=True) # needed to aggregate timestamps\n",
    "\n",
    "      # print(f\"Non silent chunks: {len(non_silent_chunks)}\")\n",
    "      \n",
    "      current_chunk = non_silent_chunks[0] if non_silent_chunks else audio_segment_file\n",
    "\n",
    "      for next_chunk in non_silent_chunks[1:]:\n",
    "        if len(current_chunk) > max_chunk_milliseconds:\n",
    "          raw_split(current_chunk)\n",
    "          current_chunk = next_chunk\n",
    "        elif len(current_chunk) + len(next_chunk) <= max_chunk_milliseconds:\n",
    "          current_chunk += next_chunk\n",
    "        else:\n",
    "          add_chunk(current_chunk)\n",
    "          current_chunk = next_chunk\n",
    "      \n",
    "      if len(current_chunk) > max_chunk_milliseconds:\n",
    "        raw_split(current_chunk)\n",
    "      else:\n",
    "        add_chunk(current_chunk)\n",
    "      \n",
    "      if options['verbose']:\n",
    "        print(f'Total chunks: {len(api_audio_chunks)}\\n')\n",
    "    else:\n",
    "      api_audio_chunks.append(api_audio_path)\n",
    "    \n",
    "    ## process chunks\n",
    "    result = None\n",
    "\n",
    "    for api_audio_chunk_path in api_audio_chunks:\n",
    "      ## API request\n",
    "      with open(api_audio_chunk_path, 'rb') as api_audio_file:\n",
    "        api_result = api_transcribe(model=api_model, file=api_audio_file, **api_options)\n",
    "        api_result = api_result.model_dump() # to dict\n",
    "      \n",
    "      api_segments = api_result['segments']\n",
    "      \n",
    "      if result:\n",
    "        ## update timestamps\n",
    "        last_segment_timestamp = result['segments'][-1]['end'] if result['segments'] else 0\n",
    "\n",
    "        for segment in api_segments:\n",
    "          segment['start'] += last_segment_timestamp\n",
    "          segment['end'] += last_segment_timestamp\n",
    "\n",
    "        ## append new segments\n",
    "        result['segments'].extend(api_segments)\n",
    "        \n",
    "        if 'duration' in result:\n",
    "          result['duration'] += api_result.get('duration', 0)\n",
    "      else:\n",
    "        ## first request\n",
    "        result = api_result\n",
    "        \n",
    "        if detect_language:\n",
    "          print(f\"Detected language: {result['language'].title()}\\n\")\n",
    "    \n",
    "      ## display segments\n",
    "      if options['verbose']:\n",
    "        for segment in api_segments:\n",
    "          print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {segment['text']}\")\n",
    "  else:\n",
    "    # Open-Source\n",
    "    result = whisper.transcribe(model, audio_path, **options)\n",
    "    output_file_name = Path(audio_path).stem.replace('.', '_') # remove spaces to make life easier\n",
    "    output_dir = os.path.dirname(audio_path)\n",
    "    for output_format in output_formats:\n",
    "      write_result(result, output_format, output_file_name)\n",
    "\n",
    "\n",
    "  # fix results formatting\n",
    "  for segment in result['segments']:\n",
    "    segment['text'] = segment['text'].strip()\n",
    "  \n",
    "  result['text'] = '\\n'.join(map(lambda segment: segment['text'], result['segments']))\n",
    "\n",
    "  # set results for this audio file\n",
    "  results[audio_path] = result\n",
    "  #os.rename(file_path_new, file_path) # restore original file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed .srt to video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srt_embedder import batch_embed\n",
    "\n",
    "# embed srt in video\n",
    "batch_embed.batch_embed_srt(\"E:\\dir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
